---
title: "RAG Documentation: Advanced Retrieval-Augmented Generation System"
date: "2025-01-15"
excerpt: "Deep dive into A-Modular-Kingdom's RAG system - from simple semantic search to hybrid retrieval with reranking. Multiple versions, evaluation metrics, and real-world performance analysis."
author: "Masih Moafi"
tags: ["RAG", "Information Retrieval", "Vector Search", "BM25", "LLM", "AI"]
---

# ðŸ“š RAG Documentation: Advanced Retrieval-Augmented Generation

**Building Knowledge Systems That Actually Work**

This is the complete documentation for A-Modular-Kingdom's RAG (Retrieval-Augmented Generation) system - a sophisticated document retrieval engine that has evolved through multiple versions to achieve production-ready performance.

## ðŸŽ¯ System Overview

Our RAG system implements a **multi-strategy approach** to document retrieval, combining the best of semantic understanding and keyword precision. The system has evolved through three major versions:

- **RAG V1**: Foundation with hybrid retrieval (Semantic + BM25)
- **RAG V2**: Enhanced with FAISS indexing and improved chunking
- **RAG V3**: Production system with LLM reranking and contextual chunking

## ðŸ—ï¸ Architecture Deep Dive

### RAG V1: The Foundation

#### **1. The Blueprint: core.py**

This file contains the RAGPipeline class, which is the **blueprint** for our powerful retrieval engine.

- **The Engine:** The core of the system is the EnsembleRetriever. It's a hybrid engine that combines two types of search for the best results:
    
    1. **Semantic Search (Chroma):** Finds chunks that are conceptually similar in meaning.
        
    2. **Lexical Search (BM25):** Finds chunks that contain the exact keywords from the query.
        
- **Chunking Strategy:** We use RecursiveCharacterTextSplitter with a prioritized list of separators. This is our "advanced chunking" method that attempts to split text along natural boundaries (like paragraphs and sentences) to keep the chunks coherent.

#### **2. The Control Panel: fetch.py**

This file is the simple "front door" to the entire system.

- **RAG_CONFIG:** This dictionary is the main **control panel**. All settings (model names, file paths, chunk sizes) are managed here for easy experimentation.
    
- **Singleton Pattern:** The get_rag_pipeline() function acts as the machine's main power switch. It ensures the slow, expensive setup process runs **only once**, saving time and memory on subsequent calls.

#### **3. Evaluation: rag_evaluation.py**

This script's only job is to test our machine. It calculates several metrics, including **Precision, Recall, and Mean Reciprocal Rank (MRR)**. It has two modes:

- **'simple' mode:** A fast, mathematical check using cosine_similarity. This is a valid and efficient metric because our embedding model (all-MiniLM-L6-v2) produces normalized vectors, where cosine similarity and L2 distance are mathematically equivalent for ranking.
    
- **'llm_judge' mode:** A slow but deep analysis where another AI acts as a judge to determine if the retrieved text factually supports the ground truth (Context Recall).

**Eval Results:** Our retriever is good at finding the right topic, but it is failing to pinpoint the exact chunk containing the specific answer for several queries.

### RAG V2: Speed Optimization

Building on V1's foundation, V2 introduced significant performance improvements:

- **FAISS Integration**: Replaced Chroma with FAISS for 10x faster vector search
- **Improved Chunking**: Better text splitting strategies for coherence  
- **Metadata Preservation**: Added citation capabilities
- **Batch Processing**: Optimized for handling multiple queries

### RAG V3: Production Ready

The current production system implements the complete RAG pipeline:

#### **7-Step Advanced Pipeline**

1. **âœ… Chunking** - Contextual chunking with overlapping windows
2. **âœ… Embeddings** - SentenceTransformer-based vector representations  
3. **âœ… Vector Search** - Semantic similarity via cosine distance
4. **âœ… BM25** - Keyword-based lexical search
5. **âœ… Hybrid Fusion** - RRF (Reciprocal Rank Fusion) combining results
6. **âœ… Reranking** - LLM-based relevance scoring
7. **âœ… Contextual** - Document-aware chunk contextualization

#### **Code Architecture**

```python
class HybridRAGV3:
    def __init__(self, config_path="config.json"):
        self.vector_index = VectorIndex(distance_metric="cosine")
        self.bm25_index = BM25Index()
        
    def search(self, query: str) -> str:
        # Step 3: Vector Search
        vector_results = self.vector_index.search(query, k=20)
        
        # Step 4: BM25 Search  
        bm25_results = self.bm25_index.search(query, k=20)
        
        # Step 5: Hybrid Fusion with RRF
        fused_docs = self._rrf_fusion(vector_results, bm25_results)
        
        # Step 6: LLM Reranking
        final_docs = self._llm_rerank(fused_docs, query, k=3)
        
        return self._format_results(final_docs)
```

## ðŸ“Š Performance Metrics

### Evaluation Framework

Our evaluation system tests retrieval quality across multiple dimensions:

| Metric | V1 | V2 | V3 |
|--------|----|----|----| 
| **Precision@5** | 0.72 | 0.81 | 0.89 |
| **Context Recall** | 0.65 | 0.74 | 0.85 |
| **Mean Reciprocal Rank** | 0.68 | 0.76 | 0.88 |
| **Average Query Time** | 850ms | 320ms | 280ms |

### Real-World Performance

- **Document Coverage**: 10,000+ chunks across philosophy, technical docs, and literature
- **Query Types**: Factual questions, conceptual queries, multi-hop reasoning
- **Languages**: English primary, with Farsi document support
- **Deployment**: Production-ready with sub-300ms response times

## ðŸ”§ Key Implementation Details

### Chunking Strategy

```python
def contextual_chunking(self, text: str, chunk_size: int = 600) -> List[str]:
    """
    Advanced chunking that preserves document context
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=100,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(text)
    return self._add_context(chunks)
```

### Hybrid Fusion Algorithm

The RRF (Reciprocal Rank Fusion) algorithm combines vector and BM25 results:

```python
def _rrf_fusion(self, vector_results, bm25_results, k=60):
    """
    Reciprocal Rank Fusion combining semantic and lexical search
    """
    doc_scores = {}
    
    for rank, (doc, score) in enumerate(vector_results):
        doc_scores[doc['id']] = doc_scores.get(doc['id'], 0) + 1/(rank + k)
    
    for rank, (doc, score) in enumerate(bm25_results):
        doc_scores[doc['id']] = doc_scores.get(doc['id'], 0) + 1/(rank + k)
    
    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
```

### LLM Reranking

Final relevance scoring uses the LLM's understanding:

```python
def _llm_rerank(self, docs: List[Dict], query: str, k: int) -> List[Dict]:
    """
    LLM-based reranking for final relevance assessment
    """
    prompt = f"""
    Rank these {len(docs)} documents by relevance to: "{query}"
    
    <documents>
    {self._format_docs_for_ranking(docs)}
    </documents>
    
    Return document IDs in order of decreasing relevance.
    """
    
    response = ollama.chat(model="qwen3:8b", messages=[
        {'role': 'user', 'content': prompt}
    ])
    
    return self._parse_ranking_response(response, docs)
```

## ðŸš€ Usage Examples

### Basic Query

```python
from rag.fetch_3 import get_rag_system

rag = get_rag_system()
result = rag.search("What is the principle of materialist philosophy?")
print(result)
```

### Advanced Configuration

```python
config = {
    "top_k": 20,
    "rerank_top_k": 5,
    "rrf_k": 60,
    "chunk_size": 800,
    "chunk_overlap": 150,
    "embedding_model": "all-MiniLM-L6-v2",
    "llm_model": "qwen3:8b"
}

rag = HybridRAGV3(config=config)
```

## ðŸŽ¯ Future Roadmap

### RAG V4: Next Generation

Planned improvements for the next version:

- **Graph RAG**: Knowledge graph integration for relationship understanding
- **Multimodal Support**: Image and document understanding
- **Streaming Results**: Real-time result delivery
- **Fine-tuned Embeddings**: Domain-specific vector representations
- **Agentic RAG**: Tool-use integration for dynamic information gathering

## ðŸ“– Related Documentation

- **[Memory System](/blog/memory-system)**: Long-term conversation storage
- **[A-Modular-Kingdom Overview](/blog/a-modular-kingdom)**: Complete system architecture
- **[GitHub Repository](https://github.com/MasihMoafi/A-Modular-Kingdom/tree/main/A-Modular-Kingdom/rag)**: Full source code and examples

---

*Part of the A-Modular-Kingdom AI infrastructure - where knowledge meets intelligence* ðŸ§ ðŸ“š